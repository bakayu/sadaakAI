{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1.0 Loading pretrained weights, using LitGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we are loading the weights using an open-source library called LitGPT\n",
    "- LitGPT is fundamentally similar to the LLM code we implemented previously, but it is much more sophisticated and supports more than 20 different LLMs (Mistral, Gemma, Llama, Phi, and more)\n",
    "\n",
    "# ⚡ LitGPT\n",
    "\n",
    "**20+ high-performance LLMs with recipes to pretrain, finetune, deploy at scale.**\n",
    "\n",
    "<pre>\n",
    "✅ From scratch implementations     ✅ No abstractions    ✅ Beginner friendly   \n",
    "✅ Flash attention                  ✅ FSDP               ✅ LoRA, QLoRA, Adapter\n",
    "✅ Reduce GPU memory (fp4/8/16/32)  ✅ 1-1000+ GPUs/TPUs  ✅ 20+ LLMs            \n",
    "</pre>\n",
    "\n",
    "## Basic usage:\n",
    "\n",
    "```\n",
    "# ligpt [action] [model]\n",
    "litgpt  download  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  chat      meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  evaluate  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  finetune  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  pretrain  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  serve     meta-llama/Meta-Llama-3-8B-Instruct\n",
    "```\n",
    "\n",
    "\n",
    "- You can learn more about LitGPT in the [corresponding GitHub repository](https://github.com/Lightning-AI/litgpt), that contains many tutorials, use cases, and examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "litgpt version: 0.4.11\n",
      "torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"litgpt\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "Please specify --repo_id <repo_id>. Available values:\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "databricks/dolly-v2-7b\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "h2oai/h2o-danube2-1.8b-chat\n",
      "keeeeenw/MicroLlama\n",
      "lmsys/longchat-13b-16k\n",
      "lmsys/longchat-7b-16k\n",
      "lmsys/vicuna-13b-v1.3\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-33b-v1.3\n",
      "lmsys/vicuna-7b-v1.3\n",
      "lmsys/vicuna-7b-v1.5\n",
      "lmsys/vicuna-7b-v1.5-16k\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "NousResearch/Nous-Hermes-13b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "model-00001-of-00002.safetensors: 100%|████| 5.00G/5.00G [02:17<00:00, 36.2MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████| 564M/564M [00:18<00:00, 31.1MB/s]\n",
      "Converting .safetensor files to PyTorch binaries (.bin)\n",
      "checkpoints/microsoft/phi-2/model-00002-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00002-of-00002.bin\n",
      "checkpoints/microsoft/phi-2/model-00001-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00001-of-00002.bin\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/microsoft/phi-2'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Loading weights: model-00002-of-00002.bin: 100%|████████| 00:13<00:00,  7.20it/s\n",
      "Saving converted checkpoint to checkpoints/microsoft/phi-2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download microsoft/phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "config.json: 100%|█████████████████████████████| 736/736 [00:00<00:00, 4.01MB/s]\n",
      "generation_config.json: 100%|█████████████████| 74.0/74.0 [00:00<00:00, 946kB/s]\n",
      "model.safetensors: 100%|███████████████████| 2.84G/2.84G [01:20<00:00, 35.2MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:01<00:00, 2.10MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 237/237 [00:00<00:00, 1.42MB/s]\n",
      "Converting .safetensor files to PyTorch binaries (.bin)\n",
      "checkpoints/microsoft/phi-1_5/model.safetensors --> checkpoints/microsoft/phi-1_5/model.bin\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/microsoft/phi-1_5'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Loading weights: model.bin: 100%|███████████████████████| 00:06<00:00, 15.11it/s\n",
      "Saving converted checkpoint to checkpoints/microsoft/phi-1_5\n"
     ]
    }
   ],
   "source": [
    "!litgpt download microsoft/phi-1_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bakayu/.conda/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"microsoft/phi-1_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "model.safetensors: 100%|███████████████████| 1.22G/1.22G [00:30<00:00, 15.9MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 1.91MB/s]\n",
      "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 32.1MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 930/930 [00:00<00:00, 5.69MB/s]\n",
      "Converting .safetensor files to PyTorch binaries (.bin)\n",
      "checkpoints/keeeeenw/MicroLlama/model.safetensors --> checkpoints/keeeeenw/MicroLlama/model.bin\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/keeeeenw/MicroLlama'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Loading weights: model.bin: 100%|███████████████████████| 00:01<00:00, 57.75it/s\n",
      "Saving converted checkpoint to checkpoints/keeeeenw/MicroLlama\n"
     ]
    }
   ],
   "source": [
    "!litgpt download keeeeenw/MicroLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bakayu/.conda/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"keeeeenw/MicroLlama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' When green sea turtles arrive in the same beaches each year to lay their eggs, they follow the normal beach conditions. They choose a beach that is exposed to plenty of sunlight to help their eggs warm up and hatch properly. They also dig themselves a simple'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(\"what do Llamas eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Red.\n",
      "\n",
      "2. What sound does a"
     ]
    }
   ],
   "source": [
    "result = llm.generate(\"what colour is an apple?\", stream=True, max_new_tokens=10)\n",
    "for e in result:\n",
    "    print(e, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
